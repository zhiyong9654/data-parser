<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>data_parser.DataSource API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>data_parser.DataSource</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import re
import glob
import multiprocessing as mp
import itertools
import pandas as pd


# Function is defined at root level to allow them to be pickleable for multiprocessing.
def regex_groups(regex, line, on_error):
    &#34;&#34;&#34;Function encapsulating regex search, for use with multiprocessing library. It will return the matching groups in a tuple, and raise errors if on_error is set to &#39;raise&#39;.
    Explanation why this function is needed:
        Instead of returning re.match objects which are not picklable, re.match.groups which are tuples are returned instead. The return of this function must be picklable for multiprocessing to work, hence this encapsulation is necessary.
        Also if raw logs are huge, we would want the error to be raised as soon as possible. Normal regex searches will only return None if regex fails to match, hence this function also raises a ValueError when on_error is set to &#39;raise&#39;

    Args:
        regex (re.compiled): Regex to match to line.
        line (str): String to parse.
        on_error (str): &#39;raise&#39; or &#39;ignore&#39;. If raise is set, ValueError will be raised when regex fails to match. If ignore is set, None value will be returned as per normal regex behaviour.

    Returns:
        match.groups() or None (tuple/None): If regex matches - match.groups() will be returned, otherwise None is returned.

    Raises:
        ValueError: ValueError is raised if on_error is set to &#39;raise&#39; and regex fails to match.
    &#34;&#34;&#34;
    if match := re.search(regex, line):
        return match.groups()
    else:
        if on_error == &#39;ignore&#39;:
            return match  # None will be returned when re.search fails.
        else:  # on_error == &#39;raise&#39;
            raise ValueError(f&#39;Regex failed to match: {line}&#39;)


class DataSource:
    def __init__(self, path, mode=&#39;local&#39;):
        &#34;&#34;&#34;
        Args:
            path (str): Glob-like filename matching of files.

        Opt args:
            mode (str): &#39;local&#39;(default) or &#39;spark&#39;. Select depending on the available backend. Local uses pandas dataframes while spark uses spark dataframes.

        Raises:
            ValueError: If an unexpected mode is passed.
        &#34;&#34;&#34;
        if mode not in [&#39;local&#39;, &#39;spark&#39;]:
            raise ValueError(f&#39;Expected &#34;local&#34; or &#34;spark&#34;, but received {mode}.&#39;)

        self.path = path
        self.mode = mode
        self._load()

    def _load(self):
        &#34;&#34;&#34;Load the data for processing.
        local - Glob match and lazily load the data.
        spark - Glob match and lazily create the rdd.
        &#34;&#34;&#34;
        if self.mode == &#39;local&#39;:
            lazy_readers = [open(filename, &#39;r&#39;) for filename in glob.glob(self.path)]
            self.data = itertools.chain(*lazy_readers)  # Concatenate multiple generators
        else:  # spark
            from pyspark.sql import SparkSession
            spark = SparkSession\
                    .builder\
                    .appName(&#39;Data_Parsing&#39;)\
                    .getOrCreate()
            sc = spark.sparkContext
            self.data = sc.textFile(self.path)

    def parse(self, regex, columns, on_error=&#39;raise&#39;):
        &#34;&#34;&#34;Parse the raw logs into a dataframe.
        Args:
            regex (str): Regex to match raw logs. Must contain capturing groups equal to number of columns.
            columns (list of str): List of column names.

        Opt Args:
            on_error (str): &#39;raise&#39;(default)/&#39;ignore&#39;. If the regex doesn&#39;t match a line, raise will raise an error. ignore will ignore it and remove the line.

        Returns:
            Dataframe: Local mode returns pandas dataframe. Spark mode returns spark dataframe.

        Raises:
            ValueError: If regex doesn&#39;t contain equal number of capturing groups as there are columns.
            ValueError: If something other than &#39;raise&#39; or &#39;ignore&#39; is passed into on_error.
        &#34;&#34;&#34;
        regex = re.compile(regex)
        if regex.groups != len(columns):
            raise ValueError(&#39;Number of regex groups and number of columns passed do not match.&#39;)

        if on_error not in [&#39;raise&#39;, &#39;ignore&#39;]:
            raise ValueError(f&#39;Expected &#34;raise&#34;/&#34;ignore&#34; but got {on_error}.&#39;)

        if self.mode == &#39;local&#39;:
            # Create an argument generator for starmap
            args = ((regex, line, on_error) for line in self.data)
            with mp.Pool() as p:
                parsed_data = p.starmap(regex_groups, args)  # Apply custom regex search function, passing in each arg.
            parsed_data = filter(bool, parsed_data)
            df = pd.DataFrame.from_records(parsed_data, columns=columns)

        elif self.mode == &#39;spark&#39;:
            df = self.data.map(lambda line: regex_groups(regex, line, on_error)).filter(bool).toDF(columns)
        return df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="data_parser.DataSource.regex_groups"><code class="name flex">
<span>def <span class="ident">regex_groups</span></span>(<span>regex, line, on_error)</span>
</code></dt>
<dd>
<div class="desc"><p>Function encapsulating regex search, for use with multiprocessing library. It will return the matching groups in a tuple, and raise errors if on_error is set to 'raise'.
Explanation why this function is needed:
Instead of returning re.match objects which are not picklable, re.match.groups which are tuples are returned instead. The return of this function must be picklable for multiprocessing to work, hence this encapsulation is necessary.
Also if raw logs are huge, we would want the error to be raised as soon as possible. Normal regex searches will only return None if regex fails to match, hence this function also raises a ValueError when on_error is set to 'raise'</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>regex</code></strong> :&ensp;<code>re.compiled</code></dt>
<dd>Regex to match to line.</dd>
<dt><strong><code>line</code></strong> :&ensp;<code>str</code></dt>
<dd>String to parse.</dd>
<dt><strong><code>on_error</code></strong> :&ensp;<code>str</code></dt>
<dd>'raise' or 'ignore'. If raise is set, ValueError will be raised when regex fails to match. If ignore is set, None value will be returned as per normal regex behaviour.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>match.groups() or None (tuple/None): If regex matches - match.groups() will be returned, otherwise None is returned.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>ValueError is raised if on_error is set to 'raise' and regex fails to match.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regex_groups(regex, line, on_error):
    &#34;&#34;&#34;Function encapsulating regex search, for use with multiprocessing library. It will return the matching groups in a tuple, and raise errors if on_error is set to &#39;raise&#39;.
    Explanation why this function is needed:
        Instead of returning re.match objects which are not picklable, re.match.groups which are tuples are returned instead. The return of this function must be picklable for multiprocessing to work, hence this encapsulation is necessary.
        Also if raw logs are huge, we would want the error to be raised as soon as possible. Normal regex searches will only return None if regex fails to match, hence this function also raises a ValueError when on_error is set to &#39;raise&#39;

    Args:
        regex (re.compiled): Regex to match to line.
        line (str): String to parse.
        on_error (str): &#39;raise&#39; or &#39;ignore&#39;. If raise is set, ValueError will be raised when regex fails to match. If ignore is set, None value will be returned as per normal regex behaviour.

    Returns:
        match.groups() or None (tuple/None): If regex matches - match.groups() will be returned, otherwise None is returned.

    Raises:
        ValueError: ValueError is raised if on_error is set to &#39;raise&#39; and regex fails to match.
    &#34;&#34;&#34;
    if match := re.search(regex, line):
        return match.groups()
    else:
        if on_error == &#39;ignore&#39;:
            return match  # None will be returned when re.search fails.
        else:  # on_error == &#39;raise&#39;
            raise ValueError(f&#39;Regex failed to match: {line}&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="data_parser.DataSource.DataSource"><code class="flex name class">
<span>class <span class="ident">DataSource</span></span>
<span>(</span><span>path, mode='local')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Glob-like filename matching of files.</dd>
</dl>
<p>Opt args:
mode (str): 'local'(default) or 'spark'. Select depending on the available backend. Local uses pandas dataframes while spark uses spark dataframes.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If an unexpected mode is passed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataSource:
    def __init__(self, path, mode=&#39;local&#39;):
        &#34;&#34;&#34;
        Args:
            path (str): Glob-like filename matching of files.

        Opt args:
            mode (str): &#39;local&#39;(default) or &#39;spark&#39;. Select depending on the available backend. Local uses pandas dataframes while spark uses spark dataframes.

        Raises:
            ValueError: If an unexpected mode is passed.
        &#34;&#34;&#34;
        if mode not in [&#39;local&#39;, &#39;spark&#39;]:
            raise ValueError(f&#39;Expected &#34;local&#34; or &#34;spark&#34;, but received {mode}.&#39;)

        self.path = path
        self.mode = mode
        self._load()

    def _load(self):
        &#34;&#34;&#34;Load the data for processing.
        local - Glob match and lazily load the data.
        spark - Glob match and lazily create the rdd.
        &#34;&#34;&#34;
        if self.mode == &#39;local&#39;:
            lazy_readers = [open(filename, &#39;r&#39;) for filename in glob.glob(self.path)]
            self.data = itertools.chain(*lazy_readers)  # Concatenate multiple generators
        else:  # spark
            from pyspark.sql import SparkSession
            spark = SparkSession\
                    .builder\
                    .appName(&#39;Data_Parsing&#39;)\
                    .getOrCreate()
            sc = spark.sparkContext
            self.data = sc.textFile(self.path)

    def parse(self, regex, columns, on_error=&#39;raise&#39;):
        &#34;&#34;&#34;Parse the raw logs into a dataframe.
        Args:
            regex (str): Regex to match raw logs. Must contain capturing groups equal to number of columns.
            columns (list of str): List of column names.

        Opt Args:
            on_error (str): &#39;raise&#39;(default)/&#39;ignore&#39;. If the regex doesn&#39;t match a line, raise will raise an error. ignore will ignore it and remove the line.

        Returns:
            Dataframe: Local mode returns pandas dataframe. Spark mode returns spark dataframe.

        Raises:
            ValueError: If regex doesn&#39;t contain equal number of capturing groups as there are columns.
            ValueError: If something other than &#39;raise&#39; or &#39;ignore&#39; is passed into on_error.
        &#34;&#34;&#34;
        regex = re.compile(regex)
        if regex.groups != len(columns):
            raise ValueError(&#39;Number of regex groups and number of columns passed do not match.&#39;)

        if on_error not in [&#39;raise&#39;, &#39;ignore&#39;]:
            raise ValueError(f&#39;Expected &#34;raise&#34;/&#34;ignore&#34; but got {on_error}.&#39;)

        if self.mode == &#39;local&#39;:
            # Create an argument generator for starmap
            args = ((regex, line, on_error) for line in self.data)
            with mp.Pool() as p:
                parsed_data = p.starmap(regex_groups, args)  # Apply custom regex search function, passing in each arg.
            parsed_data = filter(bool, parsed_data)
            df = pd.DataFrame.from_records(parsed_data, columns=columns)

        elif self.mode == &#39;spark&#39;:
            df = self.data.map(lambda line: regex_groups(regex, line, on_error)).filter(bool).toDF(columns)
        return df</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="data_parser.DataSource.DataSource.parse"><code class="name flex">
<span>def <span class="ident">parse</span></span>(<span>self, regex, columns, on_error='raise')</span>
</code></dt>
<dd>
<div class="desc"><p>Parse the raw logs into a dataframe.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>regex</code></strong> :&ensp;<code>str</code></dt>
<dd>Regex to match raw logs. Must contain capturing groups equal to number of columns.</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>List of column names.</dd>
</dl>
<p>Opt Args:
on_error (str): 'raise'(default)/'ignore'. If the regex doesn't match a line, raise will raise an error. ignore will ignore it and remove the line.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dataframe</code></dt>
<dd>Local mode returns pandas dataframe. Spark mode returns spark dataframe.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If regex doesn't contain equal number of capturing groups as there are columns.</dd>
<dt><code>ValueError</code></dt>
<dd>If something other than 'raise' or 'ignore' is passed into on_error.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(self, regex, columns, on_error=&#39;raise&#39;):
    &#34;&#34;&#34;Parse the raw logs into a dataframe.
    Args:
        regex (str): Regex to match raw logs. Must contain capturing groups equal to number of columns.
        columns (list of str): List of column names.

    Opt Args:
        on_error (str): &#39;raise&#39;(default)/&#39;ignore&#39;. If the regex doesn&#39;t match a line, raise will raise an error. ignore will ignore it and remove the line.

    Returns:
        Dataframe: Local mode returns pandas dataframe. Spark mode returns spark dataframe.

    Raises:
        ValueError: If regex doesn&#39;t contain equal number of capturing groups as there are columns.
        ValueError: If something other than &#39;raise&#39; or &#39;ignore&#39; is passed into on_error.
    &#34;&#34;&#34;
    regex = re.compile(regex)
    if regex.groups != len(columns):
        raise ValueError(&#39;Number of regex groups and number of columns passed do not match.&#39;)

    if on_error not in [&#39;raise&#39;, &#39;ignore&#39;]:
        raise ValueError(f&#39;Expected &#34;raise&#34;/&#34;ignore&#34; but got {on_error}.&#39;)

    if self.mode == &#39;local&#39;:
        # Create an argument generator for starmap
        args = ((regex, line, on_error) for line in self.data)
        with mp.Pool() as p:
            parsed_data = p.starmap(regex_groups, args)  # Apply custom regex search function, passing in each arg.
        parsed_data = filter(bool, parsed_data)
        df = pd.DataFrame.from_records(parsed_data, columns=columns)

    elif self.mode == &#39;spark&#39;:
        df = self.data.map(lambda line: regex_groups(regex, line, on_error)).filter(bool).toDF(columns)
    return df</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="data_parser" href="index.html">data_parser</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="data_parser.DataSource.regex_groups" href="#data_parser.DataSource.regex_groups">regex_groups</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="data_parser.DataSource.DataSource" href="#data_parser.DataSource.DataSource">DataSource</a></code></h4>
<ul class="">
<li><code><a title="data_parser.DataSource.DataSource.parse" href="#data_parser.DataSource.DataSource.parse">parse</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>